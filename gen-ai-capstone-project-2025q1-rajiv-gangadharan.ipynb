{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339021,"sourceType":"datasetVersion","datasetId":7093658}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Information\n\nProject Background: Building a Desktop and Laptop Support Agent Assistant with Generative AI\n\n## Context\n\nIn today's oganizations, providing efficient and effective customer support for technical issues related to desktops and laptops is crucial. Customer support agents often deal with a wide range of queries, from basic troubleshooting steps to more complex hardware and software problems. Access to well-structured and easily searchable support documentation is essential for agents to quickly diagnose and resolve customer issues.\n","metadata":{}},{"cell_type":"markdown","source":"## Problem Outline\n\nTraditional methods of accessing support documentation can be time-consuming. Agents may need to navigate through lengthy PDF files or knowledge base articles to find the relevant information. This can lead to longer resolution times, increased frustration for both agents and customers, and potentially lower customer satisfaction.","metadata":{}},{"cell_type":"markdown","source":"##  Proposed Solution\n\n\nThis project aims to leverage the power of Generative AI to create an intelligent assistant that can enhance the capabilities of customer support agents dealing with desktop and laptop issues. The core idea is to process existing support documentation (in this case, a synthetic PDF file created for this purpose) and enable agents to quickly retrieve relevant information and potentially generate helpful responses or troubleshooting steps based on customer queries.","metadata":{}},{"cell_type":"markdown","source":"# Key Components:\n\n## Support Documentation\n\nThis is the primary dataset. A synthetic PDF file containing common desktop and laptop troubleshooting steps, structured into logical sections (e.g., power issues, display problems, network connectivity, battery issues). This document serves as the knowledge base for the AI assistant.\n\n## Generative AI Model\n\nA Large Language Model (LLM), _command-r-plus_ from _COHERE_ This model will be used to understand user queries and extract relevant information from the support documentation.\n\n## Embedding Model\n\nThe embedding model used is _embed-english-light-v3.0_ also from _COHERE_. \n\n## Vector Store\n\nI have used chromadb in this case. \n\n## LangChain and Langgraph Framework\n\nThe LangChain library was used as the framework to connect the LLM with the support documentation. This involved techniques like:\n\n* Document Loading: Loading and processing the PDF file.\n* Text Splitting: Dividing the document into smaller chunks for efficient retrieval.\n* Vector Embeddings: Creating vector representations of the text chunks to enable semantic search.\n* Retrieval-Based Question Answering: Using a retrieval mechanism (e.g., a vector store and similarity search) to find relevant sections in the documentation based on the agent's query.\n* Response Generation: Utilizing the LLM to generate concise and helpful answers or troubleshooting steps based on the retrieved information.\n\nFor the agent creation and operation, LangGraph was used.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -qU langchain-core langchain-text-splitters langchain \n!pip install -qU transformers sentence-transformers  pypdf langchain\n!pip uninstall --yes chromadb\n!pip install \"chromadb<0.7.0,>=0.4.0\"\n!pip install -qU \"langchain[cohere]\"\n!pip install -qU langchain-community\n!pip install -qU \"langchain-chroma>=0.1.2\"\n!pip install  --upgrade langchain langchain-core langchain-cohere\n!pip uninstall -y ypy-websocket\n!pip install langchain-core \"langgraph>0.2.27\" \n!pip install -Uq \"unstructured-client<0.30.0\"\n!pip uninstall -y aiofiles\n!pip install \"aiofiles>=24.1.0\"\n!pip install langchain-community\n!pip install pypdf\n!pip install \"unstructured[pdf]\"\n!pip uninstall -qqy protobuf\n!pip install \"protobuf==4.25.6\"\n!pip install \"google-api-core>=2.19.1\"\n!pip uninstall --yes fsspec rich\n!pip install \"fsspec==2024.10.0\"\n!pip install \"rich<14,>=13\"\n!pip uninstall --yes toolz\n!pip install -qU \"toolz<1.0,>0.9\"\n%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_chroma import Chroma","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nCO_API_KEY = UserSecretsClient().get_secret(\"COHERE_API_KEY\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.chat_models import init_chat_model","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = init_chat_model(\"command-r-plus\", \n                        model_provider=\"cohere\",\n                        cohere_api_key=CO_API_KEY)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define loader, text splitters \nThe next step is to get the loader and the text spliters so that we can load\nand split the text document to get into a array of Documents","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders.directory import DirectoryLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loader = DirectoryLoader(path=\"..\", glob=\"**/*.pdf\", \n                         recursive=True, \n                         show_progress=True)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\ndocs: Document = loader.load_and_split()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vector Store\n\nAt this point, we have to install chromadb for the vector store and the langchain libraries which facilitate the interaction with it.","metadata":{}},{"cell_type":"code","source":"from langchain_cohere.embeddings import CohereEmbeddings\nfrom langchain_cohere.chat_models import ChatCohere\n\nembeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\", cohere_api_key=CO_API_KEY)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name = \"default_collection\",\n    embedding_function = embeddings,\n    persist_directory=\"./default_collection_db\",\n)\n\nretriever = vector_store.as_retriever()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Storing Documents into the Vector Store\n\nAt this point we have the vector store defined and have the retriever ready and \nthe next step is to generate the ids for the documents and load the documents into \nthe vector store. The next step accomplishes that. ","metadata":{}},{"cell_type":"code","source":"from uuid import uuid4\nuuids = [str(uuid4()) for _ in range(len(docs))]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"doc_ids = vector_store.add_documents(uuids = uuids, documents = docs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the similarity search \n# vector_store.similarity_search(\"monitor not working\", k=2, )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm = ChatCohere(model=\"command-r-plus\", cohere_api_key=CO_API_KEY)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\nretrieval_prompt = ChatPromptTemplate.from_template(\n    \n    \"\"\"Answer the user's question based on the context provided below:\n    \n    Context:\n    {context}\n\n    If you are unable to answer based on the document then say \"I do not have that\n    information with me.\"\n    \n    Question: {question}\n    \n    Answer:\"\"\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langgraph","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's do the agent now\nfrom typing import TypedDict, Dict\nclass AgentState(TypedDict):\n    keys: Dict[str, any]\n\n\nfrom langgraph.graph import StateGraph, END","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state:AgentState = {\"keys\":{}}\nuser_input = \"My mouse is not working, what might be the problem?\"\nstate = {\"keys\": {\"question\": user_input}}\nretriever = vector_store.as_retriever(embedding=embeddings, k = 2, )\nquestion = state[\"keys\"][\"question\"]\nretrieved_docs = retriever.get_relevant_documents(question)\nlen(retrieved_docs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"workflow = StateGraph(AgentState)\n\ndef retrieve_documents(state) -> AgentState:\n    \"\"\"Retrieves relevant documents based on the user's question.\"\"\"\n    question = state[\"keys\"][\"question\"]\n    if question is None:\n        raise KeyError(f\"No question in state {question}\")\n    print(f\"state is {state}\")\n    retriever = vector_store.as_retriever(embedding=embeddings, k = 2, )\n    retrieved_docs = retriever.get_relevant_documents(question)\n    context =  {\"context\": \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])}\n    return {\"keys\":{\"question\":question, \"context\":context,}}\n\ndef generate_response(state) -> AgentState:\n    \"\"\"Generates the final response using the Cohere model and retrieved context.\"\"\"\n    context = state[\"keys\"][\"context\"]\n    question = state[\"keys\"][\"question\"]\n    # Use the retrieval_prompt here\n    response = llm.invoke(retrieval_prompt.format_messages(context=context, question=question))\n    state[\"keys\"][\"response\"] = response\n    return state\n\n\nworkflow.add_node(\"retrieve\", retrieve_documents)\nworkflow.add_node(\"generate\", generate_response)\n\nworkflow.add_edge(\"retrieve\", \"generate\")\nworkflow.add_edge(\"generate\", END)\n\nworkflow.set_entry_point(\"retrieve\")\n\nagent = workflow.compile()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state:AgentState = {\"keys\":{\n    \"question\":\"\",\n    \"context\":\"\",\n}}\nuser_input = \"My mouse is not working, what might be the problem?\"\nstate = {\"keys\": {\"question\": user_input}}\nstate = agent.invoke(state)\n\nprint(state[\"keys\"][\"response\"])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-11T12:49:29.755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}